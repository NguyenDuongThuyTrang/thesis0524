{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMEMBER: THE GOAL IS PASS THE THESIS IN 10 DAYS, NOT TRYING TO PREDICT THE BEST\n",
    "- CORRECT DATA\n",
    "- CORRECT METHOD\n",
    "- MINIMUM EFFORT AND TIME\n",
    "- A LOT OF EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # Explicitly require this experimental feature\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LOAD DATA\n",
    "- TRAIN: UNION OF ALL DATAFRAMES (UNIQUE = KEY + TIMESTAMP)\n",
    "- TEST: NOT UNION BUT SEPERATE: DF10 ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_final_timestampadded_january23.pkl', 'rb') as f:\n",
    "    df1 = pickle.load(f)\n",
    "with open('df_final_timestampadded_february23.pkl', 'rb') as f:\n",
    "    df2 = pickle.load(f)\n",
    "with open('df_final_timestampadded_march23.pkl', 'rb') as f:\n",
    "    df3 = pickle.load(f)\n",
    "with open('df_final_timestampadded_april23.pkl', 'rb') as f:\n",
    "    df4 = pickle.load(f)\n",
    "with open('df_final_timestampadded_may23.pkl', 'rb') as f:\n",
    "    df5 = pickle.load(f)\n",
    "with open('df_final_timestampadded_june23.pkl', 'rb') as f:\n",
    "    df6 = pickle.load(f)\n",
    "with open('df_final_timestampadded_july23.pkl', 'rb') as f:\n",
    "    df7 = pickle.load(f)\n",
    "with open('df_final_timestampadded_august23.pkl', 'rb') as f:\n",
    "    df8 = pickle.load(f)\n",
    "with open('df_final_timestampadded_october23.pkl', 'rb') as f:\n",
    "    df10 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. COMBINE TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING DATA\n",
    "dfs = [df1, df2, df3, df4, df5, df6]\n",
    "# Concatenate all dataframes\n",
    "df_union_train = pd.concat(dfs, axis=0, ignore_index=True, sort=False)\n",
    "df_union_train = df_union_train.drop_duplicates()\n",
    "# Fill missing values with 0\n",
    "df_union_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union_train['key_time'] = df_union_train['key'].astype(str) + df_union_train['timestamp'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODE PATTERN_GRADE (FIT_TRANSFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguyenrn\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "data_encoded = encoder.fit_transform(df_union_train[['pattern_grade']])\n",
    "data_encoded = pd.DataFrame(data_encoded, columns=encoder.get_feature_names_out(['pattern_grade']))\n",
    "\n",
    "# Concatenate the encoded columns back to the original DataFrames\n",
    "df_union_train = pd.concat([df_union_train.reset_index(drop=True), data_encoded], axis=1)\n",
    "\n",
    "# Drop the original 'pattern_grade' as it's now encoded\n",
    "df_union_train.drop(columns=['pattern_grade'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_union_train.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data with expiry-related features\n",
    "df_union_train1 = df_union_train.drop(columns=['key','timestamp','DBMN FP SUPPLY POINT_N.V. NUTRICIA (SP)',\n",
    "                                               'pattern_grade_DO NOT USE-DO NOT USE-DO NOT USE-DO NOT USE',\n",
    "                                              'grade_w4_DO NOT USE','grade_w3_DO NOT USE','grade_w2_DO NOT USE',\n",
    "                                              'grade_w1_DO NOT USE','Country Of Origin_NL']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X1 = df_union_train1.drop('class', axis=1)\n",
    "y1 = df_union_train1['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data without expiry-related features\n",
    "df_union_train2 = df_union_train.drop(columns=['aging','remaining_shelflife','key','timestamp','DBMN FP SUPPLY POINT_N.V. NUTRICIA (SP)',\n",
    "                                               'pattern_grade_DO NOT USE-DO NOT USE-DO NOT USE-DO NOT USE',\n",
    "                                              'grade_w4_DO NOT USE','grade_w3_DO NOT USE','grade_w2_DO NOT USE',\n",
    "                                              'grade_w1_DO NOT USE','Country Of Origin_NL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X2 = df_union_train2.drop('class', axis=1)\n",
    "y2 = df_union_train2['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_union_train2.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TESTING DF7 (HOLDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING DATA: USE DF7 first\n",
    "#dft = [df7, df8, df10]\n",
    "# Concatenate all dataframes\n",
    "#df_join = pd.concat(dft, axis=0, ignore_index=True, sort=False)\n",
    "#df_join = df7.drop_duplicates(inplace=True)\n",
    "\n",
    "# Fill missing values with 0\n",
    "df7.fillna(0, inplace=True)\n",
    "df7['key_time'] = df7['key'].astype(str) + df7['timestamp'].astype(str)\n",
    "\n",
    "X_holdout_df7 = df7.drop('class', axis=1)\n",
    "y_holdout_df7 = df7['class']\n",
    "\n",
    "data_encoded_holdout = encoder.transform(X_holdout_df7[['pattern_grade']])\n",
    "data_encoded_holdout = pd.DataFrame(data_encoded_holdout, columns=encoder.get_feature_names_out(['pattern_grade']))\n",
    "\n",
    "# Concatenate the encoded columns back to the original DataFrames\n",
    "X_holdout_df7 = pd.concat([X_holdout_df7.reset_index(drop=True), data_encoded_holdout], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP 1. EXPIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_holdout_df7 = X_holdout_df7.drop(columns=['pattern_grade','key','timestamp','DBMN FP SUPPLY POINT_N.V. NUTRICIA (SP)',\n",
    "                                               'pattern_grade_DO NOT USE-DO NOT USE-DO NOT USE-DO NOT USE',\n",
    "                                              'grade_w4_DO NOT USE','grade_w3_DO NOT USE','grade_w2_DO NOT USE',\n",
    "                                              'grade_w1_DO NOT USE','Country Of Origin_NL'])\n",
    "X1, X1_holdout_df7 = X1.align(X1_holdout_df7, join='outer', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_holdout_df7 = y_holdout_df7.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP 2. NON_EXPIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test data without expiry-related features\n",
    "X2_holdout_df7 = X_holdout_df7.drop(columns=['pattern_grade','aging','remaining_shelflife','key','timestamp','DBMN FP SUPPLY POINT_N.V. NUTRICIA (SP)',\n",
    "                                               'pattern_grade_DO NOT USE-DO NOT USE-DO NOT USE-DO NOT USE',\n",
    "                                              'grade_w4_DO NOT USE','grade_w3_DO NOT USE','grade_w2_DO NOT USE',\n",
    "                                              'grade_w1_DO NOT USE','Country Of Origin_NL'])\n",
    "\n",
    "X2, X2_holdout_df7 = X2.align(X2_holdout_df7, join='outer', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_holdout_df7 = y_holdout_df7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"key_label_counts = df_union_train1.groupby('key_time')['class'].nunique()\\n\\n# Step 2: Filter to get only keys with exactly 2 unique 'label' values\\nkeys_with_two_labels = key_label_counts[key_label_counts == 2].index\\n\\n# Step 3: Filter the original DataFrame to only include rows with these keys\\nfiltered_df = df_union_train1[df_union_train1['key'].isin(keys_with_two_labels)]\\n\\n# Display the resulting DataFrame\\nfiltered_df\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''key_label_counts = df_union_train1.groupby('key_time')['class'].nunique()\n",
    "\n",
    "# Step 2: Filter to get only keys with exactly 2 unique 'label' values\n",
    "keys_with_two_labels = key_label_counts[key_label_counts == 2].index\n",
    "\n",
    "# Step 3: Filter the original DataFrame to only include rows with these keys\n",
    "filtered_df = df_union_train1[df_union_train1['key'].isin(keys_with_two_labels)]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "filtered_df'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TRAIN MODEL RF1 (USED PREVIOUS PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. RF GROUP 1. EXPIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Implement TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Placeholder lists for storing performance metrics\n",
    "accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "# Step 3: Train and validate the model using the splits\n",
    "for train_index, test_index in tscv.split(X1):\n",
    "    X_train, X_test = X1.iloc[train_index], X1.iloc[test_index]\n",
    "    y_train, y_test = y1.iloc[train_index], y1.iloc[test_index]\n",
    "\n",
    "    # Create and fit the model\n",
    "    model_rf1 = RandomForestClassifier(bootstrap=True,criterion='entropy',max_depth=None,max_features=None,min_samples_leaf=1,\n",
    "                                   min_samples_split=15,n_estimators=120,random_state=42)\n",
    "    model_rf1.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    predictions_rf1 = model_rf1.predict(X_test)\n",
    "\n",
    "    # Step 4: Evaluate performance\n",
    "    accuracies.append(accuracy_score(y_test, predictions_rf1))\n",
    "    precisions.append(precision_score(y_test, predictions_rf1, average='macro'))\n",
    "    recalls.append(recall_score(y_test, predictions_rf1, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, predictions_rf1, average='macro'))\n",
    "\n",
    "# Display average of performance metrics across all folds\n",
    "print(f'Average Accuracy: {sum(accuracies) / len(accuracies)}')\n",
    "print(f'Average Precision: {sum(precisions) / len(precisions)}')\n",
    "print(f'Average Recall: {sum(recalls) / len(recalls)}')\n",
    "print(f'Average F1 Score: {sum(f1_scores) / len(f1_scores)}')\n",
    "print(accuracies)\n",
    "print(precisions)\n",
    "print(recalls)\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. RF2 GROUP 2: NON_EXPIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Implement TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Placeholder lists for storing performance metrics\n",
    "accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "# Step 3: Train and validate the model using the splits\n",
    "for train_index, test_index in tscv.split(X2):\n",
    "    X_train, X_test = X2.iloc[train_index], X2.iloc[test_index]\n",
    "    y_train, y_test = y2.iloc[train_index], y2.iloc[test_index]\n",
    "\n",
    "    # Create and fit the model\n",
    "    model_rf2 = RandomForestClassifier(bootstrap=True,criterion='entropy',max_depth=None,max_features=None,min_samples_leaf=1,\n",
    "                                   min_samples_split=15,n_estimators=120,random_state=42)\n",
    "    model_rf2.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    predictions_rf2 = model_rf2.predict(X_test)\n",
    "\n",
    "    # Step 4: Evaluate performance\n",
    "    accuracies.append(accuracy_score(y_test, predictions_rf2))\n",
    "    precisions.append(precision_score(y_test, predictions_rf2, average='macro'))\n",
    "    recalls.append(recall_score(y_test, predictions_rf2, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, predictions_rf2, average='macro'))\n",
    "\n",
    "# Display average of performance metrics across all folds\n",
    "print(f'Average Accuracy: {sum(accuracies) / len(accuracies)}')\n",
    "print(f'Average Precision: {sum(precisions) / len(precisions)}')\n",
    "print(f'Average Recall: {sum(recalls) / len(recalls)}')\n",
    "print(f'Average F1 Score: {sum(f1_scores) / len(f1_scores)}')\n",
    "print(accuracies)\n",
    "print(precisions)\n",
    "print(recalls)\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. PREDICT RF1 ON TEST_RF => RESULT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. RESULT RF1 (EXPIRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_pred_df7 = model_rf1.predict(X1_holdout_df7)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y1_holdout_df7, y1_pred_df7)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. RESULT RF2 (NON_EXPIRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred_df7 = model_rf2.predict(X2_holdout_df7)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y2_holdout_df7, y2_pred_df7)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. FEATURE IMPORTANCE (VERSION BASIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2. FEATURE IMPORTANCE (SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. OPTIONAL: IF HAVE TIME, RUN OPTUNA AGAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN OPTUNA ON GROUP 2 (NON_EXPIRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna\n",
    "import optuna\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters to be tuned\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 700)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 50)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "    \n",
    "    # Create and configure the RandomForestClassifier model\n",
    "    model_optuna2 = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        criterion=criterion,\n",
    "        max_features=max_features,\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Perform TimeSeriesSplit cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "               'precision': make_scorer(precision_score, zero_division=0, average='macro'),\n",
    "               'recall': make_scorer(recall_score, zero_division=0, average='macro'),\n",
    "               'f1': make_scorer(f1_score, zero_division=0, average='macro')}\n",
    "\n",
    "    # Perform cross-validation\n",
    "    results_optuna2 = cross_validate(model_optuna2, X2, y2, cv=tscv, scoring=scoring)\n",
    "\n",
    "    # Combine results to get an overall average score\n",
    "    # This example takes a simple average of all scores, adjust weighting as necessary\n",
    "    avg_recall = np.mean(results_optuna2['test_recall'])\n",
    "\n",
    "    # Return the negative of the composite score to minimize\n",
    "    return -avg_recall\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study_optuna2 = optuna.create_study(direction='minimize')\n",
    "study_optuna2.optimize(objective, n_trials=50)\n",
    "\n",
    "# Output the best results\n",
    "print(\"Best parameters:\", study_optuna2.best_params)\n",
    "print(\"Best score: \", -study_optuna2.best_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
