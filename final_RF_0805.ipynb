{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMEMBER: THE GOAL IS PASS THE THESIS IN 10 DAYS, NOT TRYING TO PREDICT THE BEST\n",
    "- CORRECT DATA\n",
    "- CORRECT METHOD\n",
    "- MINIMUM EFFORT AND TIME\n",
    "- A LOT OF EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.035107Z",
     "start_time": "2024-05-08T21:35:18.839765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # Explicitly require this experimental feature\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LOAD DATA\n",
    "- TRAIN: UNION OF ALL DATAFRAMES (UNIQUE = KEY + TIMESTAMP)\n",
    "- TEST: NOT UNION BUT SEPERATE: DF10 ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.080079Z",
     "start_time": "2024-05-08T21:35:20.037305Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('df_final_timestampadded_january23.pkl', 'rb') as f:\n",
    "    df1 = pickle.load(f)\n",
    "with open('df_final_timestampadded_february23.pkl', 'rb') as f:\n",
    "    df2 = pickle.load(f)\n",
    "with open('df_final_timestampadded_march23.pkl', 'rb') as f:\n",
    "    df3 = pickle.load(f)\n",
    "with open('df_final_timestampadded_april23.pkl', 'rb') as f:\n",
    "    df4 = pickle.load(f)\n",
    "with open('df_final_timestampadded_may23.pkl', 'rb') as f:\n",
    "    df5 = pickle.load(f)\n",
    "with open('df_final_timestampadded_june23.pkl', 'rb') as f:\n",
    "    df6 = pickle.load(f)\n",
    "with open('df_final_timestampadded_july23.pkl', 'rb') as f:\n",
    "    df7 = pickle.load(f)\n",
    "with open('df_final_timestampadded_august23.pkl', 'rb') as f:\n",
    "    df8 = pickle.load(f)\n",
    "with open('df_final_timestampadded_october23.pkl', 'rb') as f:\n",
    "    df10 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.083285Z",
     "start_time": "2024-05-08T21:35:20.081104Z"
    }
   },
   "outputs": [],
   "source": [
    "#df2.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. COMBINE TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.286981Z",
     "start_time": "2024-05-08T21:35:20.085068Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAINING DATA\n",
    "dfs = [df1, df2, df3, df4, df5, df6]\n",
    "# Concatenate all dataframes\n",
    "df_union_train = pd.concat(dfs, axis=0, ignore_index=True, sort=False)\n",
    "df_union_train = df_union_train.drop_duplicates()\n",
    "# Fill missing values with 0\n",
    "df_union_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.289724Z",
     "start_time": "2024-05-08T21:35:20.287837Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_union_train['key_time'] = df_union_train['key'].astype(str) + df_union_train['timestamp'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODE PATTERN_GRADE (FIT_TRANSFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.367488Z",
     "start_time": "2024-05-08T21:35:20.290451Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "data_encoded = encoder.fit_transform(df_union_train[['pattern_grade']])\n",
    "data_encoded = pd.DataFrame(data_encoded, columns=encoder.get_feature_names_out(['pattern_grade']))\n",
    "\n",
    "# Concatenate the encoded columns back to the original DataFrames\n",
    "df_union_train = pd.concat([df_union_train.reset_index(drop=True), data_encoded], axis=1)\n",
    "\n",
    "# Drop the original 'pattern_grade' as it's now encoded\n",
    "df_union_train.drop(columns=['pattern_grade'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.370523Z",
     "start_time": "2024-05-08T21:35:20.368539Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_union_train.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.385797Z",
     "start_time": "2024-05-08T21:35:20.371312Z"
    }
   },
   "outputs": [],
   "source": [
    "#training data with expiry-related features\n",
    "df_union_train1 = df_union_train.drop(columns=['key','timestamp','DBMN FP SUPPLY POINT_N.V. NUTRICIA (SP)',\n",
    "                                               'pattern_grade_DO NOT USE-DO NOT USE-DO NOT USE-DO NOT USE',\n",
    "                                              'grade_w4_DO NOT USE','grade_w3_DO NOT USE','grade_w2_DO NOT USE',\n",
    "                                              'grade_w1_DO NOT USE','Country Of Origin_NL']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.404219Z",
     "start_time": "2024-05-08T21:35:20.386787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X1 = df_union_train1.drop('class', axis=1)\n",
    "y1 = df_union_train1['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.424025Z",
     "start_time": "2024-05-08T21:35:20.407126Z"
    }
   },
   "outputs": [],
   "source": [
    "#training data without expiry-related features\n",
    "df_union_train2 = df_union_train.drop(columns=['aging','remaining_shelflife','key','timestamp','DBMN FP SUPPLY POINT_N.V. NUTRICIA (SP)',\n",
    "                                               'pattern_grade_DO NOT USE-DO NOT USE-DO NOT USE-DO NOT USE',\n",
    "                                              'grade_w4_DO NOT USE','grade_w3_DO NOT USE','grade_w2_DO NOT USE',\n",
    "                                              'grade_w1_DO NOT USE','Country Of Origin_NL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.444259Z",
     "start_time": "2024-05-08T21:35:20.424730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X2 = df_union_train2.drop('class', axis=1)\n",
    "y2 = df_union_train2['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.447227Z",
     "start_time": "2024-05-08T21:35:20.445312Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_union_train2.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TESTING DF7 (HOLDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.461785Z",
     "start_time": "2024-05-08T21:35:20.448009Z"
    }
   },
   "outputs": [],
   "source": [
    "#TESTING DATA: USE DF7 first\n",
    "#dft = [df7, df8, df10]\n",
    "# Concatenate all dataframes\n",
    "#df_join = pd.concat(dft, axis=0, ignore_index=True, sort=False)\n",
    "#df_join = df7.drop_duplicates(inplace=True)\n",
    "\n",
    "# Fill missing values with 0\n",
    "df7.fillna(0, inplace=True)\n",
    "# df7['key_time'] = df7['key'].astype(str) + df7['timestamp'].astype(str)\n",
    "\n",
    "X_holdout_df7 = df7.drop('class', axis=1)\n",
    "y_holdout_df7 = df7['class']\n",
    "\n",
    "data_encoded_holdout = encoder.transform(X_holdout_df7[['pattern_grade']])\n",
    "data_encoded_holdout = pd.DataFrame(data_encoded_holdout, columns=encoder.get_feature_names_out(['pattern_grade']))\n",
    "\n",
    "# Concatenate the encoded columns back to the original DataFrames\n",
    "X_holdout_df7 = pd.concat([X_holdout_df7.reset_index(drop=True), data_encoded_holdout], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP 1. EXPIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.501595Z",
     "start_time": "2024-05-08T21:35:20.462720Z"
    }
   },
   "outputs": [],
   "source": [
    "X1_holdout_df7 = X_holdout_df7.drop(columns=['pattern_grade','key','timestamp','DBMN FP SUPPLY POINT_N.V. NUTRICIA (SP)',\n",
    "                                               'pattern_grade_DO NOT USE-DO NOT USE-DO NOT USE-DO NOT USE',\n",
    "                                              'grade_w4_DO NOT USE','grade_w3_DO NOT USE','grade_w2_DO NOT USE',\n",
    "                                              'grade_w1_DO NOT USE','Country Of Origin_NL'])\n",
    "X1, X1_holdout_df7 = X1.align(X1_holdout_df7, join='outer', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.504946Z",
     "start_time": "2024-05-08T21:35:20.502452Z"
    }
   },
   "outputs": [],
   "source": [
    "y1_holdout_df7 = y_holdout_df7.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP 2. NON_EXPIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.541305Z",
     "start_time": "2024-05-08T21:35:20.505811Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#test data without expiry-related features\n",
    "X2_holdout_df7 = X_holdout_df7.drop(columns=['pattern_grade','aging','remaining_shelflife','key','timestamp','DBMN FP SUPPLY POINT_N.V. NUTRICIA (SP)',\n",
    "                                               'pattern_grade_DO NOT USE-DO NOT USE-DO NOT USE-DO NOT USE',\n",
    "                                              'grade_w4_DO NOT USE','grade_w3_DO NOT USE','grade_w2_DO NOT USE',\n",
    "                                              'grade_w1_DO NOT USE','Country Of Origin_NL'])\n",
    "\n",
    "X2, X2_holdout_df7 = X2.align(X2_holdout_df7, join='outer', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.544684Z",
     "start_time": "2024-05-08T21:35:20.542255Z"
    }
   },
   "outputs": [],
   "source": [
    "y2_holdout_df7 = y_holdout_df7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:35:20.550041Z",
     "start_time": "2024-05-08T21:35:20.545658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"key_label_counts = df_union_train1.groupby('key_time')['class'].nunique()\\n\\n# Step 2: Filter to get only keys with exactly 2 unique 'label' values\\nkeys_with_two_labels = key_label_counts[key_label_counts == 2].index\\n\\n# Step 3: Filter the original DataFrame to only include rows with these keys\\nfiltered_df = df_union_train1[df_union_train1['key'].isin(keys_with_two_labels)]\\n\\n# Display the resulting DataFrame\\nfiltered_df\""
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''key_label_counts = df_union_train1.groupby('key_time')['class'].nunique()\n",
    "\n",
    "# Step 2: Filter to get only keys with exactly 2 unique 'label' values\n",
    "keys_with_two_labels = key_label_counts[key_label_counts == 2].index\n",
    "\n",
    "# Step 3: Filter the original DataFrame to only include rows with these keys\n",
    "filtered_df = df_union_train1[df_union_train1['key'].isin(keys_with_two_labels)]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "filtered_df'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TRAIN MODEL RF1 (USED PREVIOUS PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. RF GROUP 1. EXPIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:39:04.457256Z",
     "start_time": "2024-05-08T21:35:20.550898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.8885543879579665\n",
      "Average Precision: 0.8890207353034081\n",
      "Average Recall: 0.8887841322631317\n",
      "Average F1 Score: 0.8863720195243896\n",
      "[0.8863959102527691, 0.955978415222948, 0.8869639307015053, 0.8599829593865379, 0.8534507242260722]\n",
      "[0.8793253605113835, 0.9504414075293202, 0.8893126713190407, 0.860381311178681, 0.865642925978615]\n",
      "[0.8822935986979342, 0.9550024288208911, 0.8828540342318985, 0.859997134890516, 0.8637734646744182]\n",
      "[0.8807365677061316, 0.9526309645271451, 0.8851122906478897, 0.8599475323545773, 0.8534327423862045]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Implement TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Placeholder lists for storing performance metrics\n",
    "accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "# Step 3: Train and validate the model using the splits\n",
    "for train_index, test_index in tscv.split(X1):\n",
    "    X_train, X_test = X1.iloc[train_index], X1.iloc[test_index]\n",
    "    y_train, y_test = y1.iloc[train_index], y1.iloc[test_index]\n",
    "\n",
    "    # Create and fit the model\n",
    "    model_rf1 = RandomForestClassifier(bootstrap=True,criterion='entropy',max_depth=None,max_features=None,min_samples_leaf=1,\n",
    "                                   min_samples_split=15,n_estimators=120,random_state=42)\n",
    "    model_rf1.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    predictions_rf1 = model_rf1.predict(X_test)\n",
    "\n",
    "    # Step 4: Evaluate performance\n",
    "    accuracies.append(accuracy_score(y_test, predictions_rf1))\n",
    "    precisions.append(precision_score(y_test, predictions_rf1, average='macro'))\n",
    "    recalls.append(recall_score(y_test, predictions_rf1, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, predictions_rf1, average='macro'))\n",
    "\n",
    "# Display average of performance metrics across all folds\n",
    "print(f'Average Accuracy: {sum(accuracies) / len(accuracies)}')\n",
    "print(f'Average Precision: {sum(precisions) / len(precisions)}')\n",
    "print(f'Average Recall: {sum(recalls) / len(recalls)}')\n",
    "print(f'Average F1 Score: {sum(f1_scores) / len(f1_scores)}')\n",
    "print(accuracies)\n",
    "print(precisions)\n",
    "print(recalls)\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_rf1_holdout = RandomForestClassifier(bootstrap=True,criterion='entropy',max_depth=None,max_features=None,min_samples_leaf=1,\n",
    "                                   min_samples_split=15,n_estimators=120,random_state=42)\n",
    "model_rf1_holdout.fit(X1, y1)\n",
    "\n",
    "model_rf1_holdout_predictions = model_rf1_holdout.predict(X1_holdout_df7)\n",
    "\n",
    "holdout_accuracy = accuracy_score(y1_holdout_df7, model_rf1_holdout_predictions)\n",
    "holdout_precision = precision_score(y1_holdout_df7, model_rf1_holdout_predictions, average='macro')\n",
    "holdout_recalls = recall_score(y1_holdout_df7, model_rf1_holdout_predictions, average='macro')\n",
    "holdout_f1_score = f1_score(y1_holdout_df7, model_rf1_holdout_predictions, average='macro')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T21:53:16.484514Z",
     "start_time": "2024-05-08T21:51:22.600956Z"
    }
   },
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Accuracy: 0.6928406466512702\n",
      "Holdout Precision: 0.6943150924844289\n",
      "Holdout Recall: 0.715796277367026\n",
      "Holdout  F1 Score: 0.6854587935429057\n"
     ]
    }
   ],
   "source": [
    "print(f'Holdout Accuracy: {holdout_accuracy}')\n",
    "print(f'Holdout Precision: {holdout_precision}')\n",
    "print(f'Holdout Recall: {holdout_recalls}')\n",
    "print(f'Holdout  F1 Score: {holdout_f1_score}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T22:00:13.193212Z",
     "start_time": "2024-05-08T22:00:13.189914Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. RF2 GROUP 2: NON_EXPIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:43:13.534383Z",
     "start_time": "2024-05-08T21:39:04.458245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.8692984947458108\n",
      "Average Precision: 0.8678647210693009\n",
      "Average Recall: 0.8673735885426437\n",
      "Average F1 Score: 0.8665404855198829\n",
      "[0.871059358136893, 0.9474581084919057, 0.867651235444476, 0.8406702641295086, 0.819653507526271]\n",
      "[0.8638570754732988, 0.9419964627548898, 0.8712471455189902, 0.8410226142338548, 0.8212003073654711]\n",
      "[0.8646381061009509, 0.944774684986118, 0.8623371308728438, 0.8406839416421079, 0.8244340791111975]\n",
      "[0.8642424746834798, 0.943351604615359, 0.8650915260837335, 0.840632779392514, 0.819384042824328]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Implement TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Placeholder lists for storing performance metrics\n",
    "accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "# Step 3: Train and validate the model using the splits\n",
    "for train_index, test_index in tscv.split(X2):\n",
    "    X_train, X_test = X2.iloc[train_index], X2.iloc[test_index]\n",
    "    y_train, y_test = y2.iloc[train_index], y2.iloc[test_index]\n",
    "\n",
    "    # Create and fit the model\n",
    "    model_rf2 = RandomForestClassifier(bootstrap=True,criterion='entropy',max_depth=None,max_features=None,min_samples_leaf=1,\n",
    "                                   min_samples_split=15,n_estimators=120,random_state=42)\n",
    "    model_rf2.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    predictions_rf2 = model_rf2.predict(X_test)\n",
    "\n",
    "    # Step 4: Evaluate performance\n",
    "    accuracies.append(accuracy_score(y_test, predictions_rf2))\n",
    "    precisions.append(precision_score(y_test, predictions_rf2, average='macro'))\n",
    "    recalls.append(recall_score(y_test, predictions_rf2, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, predictions_rf2, average='macro'))\n",
    "\n",
    "# Display average of performance metrics across all folds\n",
    "print(f'Average Accuracy: {sum(accuracies) / len(accuracies)}')\n",
    "print(f'Average Precision: {sum(precisions) / len(precisions)}')\n",
    "print(f'Average Recall: {sum(recalls) / len(recalls)}')\n",
    "print(f'Average F1 Score: {sum(f1_scores) / len(f1_scores)}')\n",
    "print(accuracies)\n",
    "print(precisions)\n",
    "print(recalls)\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_rf2_holdout = RandomForestClassifier(bootstrap=True,criterion='entropy',max_depth=None,max_features=None,min_samples_leaf=1,\n",
    "                                   min_samples_split=15,n_estimators=120,random_state=42)\n",
    "model_rf2_holdout.fit(X2, y2)\n",
    "\n",
    "model_rf2_holdout_predictions = model_rf2_holdout.predict(X2_holdout_df7)\n",
    "\n",
    "holdout_accuracy = accuracy_score(y1_holdout_df7, model_rf2_holdout_predictions)\n",
    "holdout_precision = precision_score(y1_holdout_df7, model_rf2_holdout_predictions, average='macro')\n",
    "holdout_recalls = recall_score(y1_holdout_df7, model_rf2_holdout_predictions, average='macro')\n",
    "holdout_f1_score = f1_score(y1_holdout_df7, model_rf2_holdout_predictions, average='macro')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T22:06:04.993421Z",
     "start_time": "2024-05-08T22:04:00.487941Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Accuracy: 0.6747498075442648\n",
      "Holdout Precision: 0.681757517323923\n",
      "Holdout Recall: 0.7015058240598802\n",
      "Holdout  F1 Score: 0.6685613623691964\n"
     ]
    }
   ],
   "source": [
    "print(f'Holdout Accuracy: {holdout_accuracy}')\n",
    "print(f'Holdout Precision: {holdout_precision}')\n",
    "print(f'Holdout Recall: {holdout_recalls}')\n",
    "print(f'Holdout  F1 Score: {holdout_f1_score}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T22:06:04.998030Z",
     "start_time": "2024-05-08T22:06:04.995030Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. PREDICT RF1 ON TEST_RF => RESULT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. RESULT RF1 (EXPIRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:43:13.582398Z",
     "start_time": "2024-05-08T21:43:13.535648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.69      0.76      1709\n",
      "         1.0       0.56      0.77      0.65       889\n",
      "\n",
      "    accuracy                           0.72      2598\n",
      "   macro avg       0.71      0.73      0.71      2598\n",
      "weighted avg       0.75      0.72      0.72      2598\n"
     ]
    }
   ],
   "source": [
    "y1_pred_df7 = model_rf1.predict(X1_holdout_df7)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y1_holdout_df7, y1_pred_df7)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. RESULT RF2 (NON_EXPIRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T21:43:13.629379Z",
     "start_time": "2024-05-08T21:43:13.583225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.69      0.76      1709\n",
      "         1.0       0.56      0.76      0.65       889\n",
      "\n",
      "    accuracy                           0.72      2598\n",
      "   macro avg       0.71      0.73      0.71      2598\n",
      "weighted avg       0.75      0.72      0.72      2598\n"
     ]
    }
   ],
   "source": [
    "y2_pred_df7 = model_rf2.predict(X2_holdout_df7)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y2_holdout_df7, y2_pred_df7)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. FEATURE IMPORTANCE (VERSION BASIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2. FEATURE IMPORTANCE (SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. OPTIONAL: IF HAVE TIME, RUN OPTUNA AGAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN OPTUNA ON GROUP 2 (NON_EXPIRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T22:42:54.879606Z",
     "start_time": "2024-05-08T22:33:55.050057Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 00:33:55,055] A new study created in memory with name: no-name-a5722497-669f-4b55-8800-28b25c3a99e5\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:8: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 50, 700, 200)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [50, 700] and step=200, but the range is not divisible by `step`. It will be replaced by [50, 650].\n",
      "  warnings.warn(\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:9: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  max_depth = trial.suggest_int('max_depth', 2, 50, 15)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [2, 50] and step=15, but the range is not divisible by `step`. It will be replaced by [2, 47].\n",
      "  warnings.warn(\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:10: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  min_samples_split = trial.suggest_int('min_samples_split', 2, 20, 5)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [2, 20] and step=5, but the range is not divisible by `step`. It will be replaced by [2, 17].\n",
      "  warnings.warn(\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:11: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20, 5)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [1, 20] and step=5, but the range is not divisible by `step`. It will be replaced by [1, 16].\n",
      "  warnings.warn(\n",
      "[I 2024-05-09 00:42:47,045] Trial 0 finished with value: -0.8306540715860938 and parameters: {'n_estimators': 250, 'max_depth': 47, 'min_samples_split': 12, 'min_samples_leaf': 16, 'criterion': 'entropy', 'max_features': None, 'bootstrap': False}. Best is trial 0 with value: -0.8306540715860938.\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:8: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 50, 700, 200)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [50, 700] and step=200, but the range is not divisible by `step`. It will be replaced by [50, 650].\n",
      "  warnings.warn(\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:9: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  max_depth = trial.suggest_int('max_depth', 2, 50, 15)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [2, 50] and step=15, but the range is not divisible by `step`. It will be replaced by [2, 47].\n",
      "  warnings.warn(\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:10: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  min_samples_split = trial.suggest_int('min_samples_split', 2, 20, 5)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [2, 20] and step=5, but the range is not divisible by `step`. It will be replaced by [2, 17].\n",
      "  warnings.warn(\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:11: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20, 5)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [1, 20] and step=5, but the range is not divisible by `step`. It will be replaced by [1, 16].\n",
      "  warnings.warn(\n",
      "[I 2024-05-09 00:42:54,244] Trial 1 finished with value: -0.8281957512881164 and parameters: {'n_estimators': 250, 'max_depth': 17, 'min_samples_split': 17, 'min_samples_leaf': 6, 'criterion': 'entropy', 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: -0.8306540715860938.\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:8: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 50, 700, 200)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [50, 700] and step=200, but the range is not divisible by `step`. It will be replaced by [50, 650].\n",
      "  warnings.warn(\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:9: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  max_depth = trial.suggest_int('max_depth', 2, 50, 15)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [2, 50] and step=15, but the range is not divisible by `step`. It will be replaced by [2, 47].\n",
      "  warnings.warn(\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:10: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  min_samples_split = trial.suggest_int('min_samples_split', 2, 20, 5)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [2, 20] and step=5, but the range is not divisible by `step`. It will be replaced by [2, 17].\n",
      "  warnings.warn(\n",
      "/var/folders/jm/xcfp5lx50jl_w8_pt29cpkqh0000gn/T/ipykernel_5435/4229620254.py:11: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20, 5)\n",
      "/Users/viethnguyen/Documents/CEU_BA_MSc/capstone_cua_baby_boo/venv/lib/python3.9/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [1, 20] and step=5, but the range is not divisible by `step`. It will be replaced by [1, 16].\n",
      "  warnings.warn(\n",
      "[I 2024-05-09 00:42:54,877] Trial 2 finished with value: -0.5447895868995981 and parameters: {'n_estimators': 50, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 6, 'criterion': 'gini', 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: -0.8306540715860938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 250, 'max_depth': 47, 'min_samples_split': 12, 'min_samples_leaf': 16, 'criterion': 'entropy', 'max_features': None, 'bootstrap': False}\n",
      "Best score:  0.8306540715860938\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters to be tuned\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 700, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 50, 15)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20, 5)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20, 5)\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "    \n",
    "    # Create and configure the RandomForestClassifier model\n",
    "    model_optuna2 = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        criterion=criterion,\n",
    "        max_features=max_features,\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Perform TimeSeriesSplit cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "               'precision': make_scorer(precision_score, zero_division=0, average='macro'),\n",
    "               'recall': make_scorer(recall_score, zero_division=0, average='macro'),\n",
    "               'f1': make_scorer(f1_score, zero_division=0, average='macro')}\n",
    "\n",
    "    # Perform cross-validation\n",
    "    results_optuna2 = cross_validate(model_optuna2, X2, y2, cv=tscv, scoring=scoring)\n",
    "\n",
    "    # Combine results to get an overall average score\n",
    "    # This example takes a simple average of all scores, adjust weighting as necessary\n",
    "    avg_recall = np.mean(results_optuna2['test_recall'])\n",
    "\n",
    "    # Return the negative of the composite score to minimize\n",
    "    return -avg_recall\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study_optuna2 = optuna.create_study(direction='minimize')\n",
    "study_optuna2.optimize(objective, n_trials=3)\n",
    "\n",
    "# Output the best results\n",
    "print(\"Best parameters:\", study_optuna2.best_params)\n",
    "print(\"Best score: \", -study_optuna2.best_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
